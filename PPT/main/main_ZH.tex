% \documentclass{beamer}
\documentclass[UTF8]{ctexbeamer}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra,color}
% \usepackage[UTF8]{ctex}
\usetheme{boxes}

\title{A Brief Introduction to Me and My Research}
\author{石雨凌}
\institute{上海财经大学}
\date{2021.7.26}

\subject{Theoretical Computer Science}
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{目录}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{目录}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

\section{个人简介}

\begin{frame}{个人简介}
  \begin{itemize}
    \item 上海财经大学, 数学与应用数学专业, 年级排名 \textbf{13/104}, 本学期成绩排名 \textbf{1/104}. TOEFL 102 分. \\ \
    \item 主要课程: \textbf{数学分析 (3.3), 高等代数 (4.0), 概率论 (4.0), 文本挖掘 (4.0), 深度学习 (4.0), 人工智能 (4.0).} \\ \
    \item 完成多项自然语言处理相关研究项目, 且有一篇计算数学论文已发表在 SCI 一区杂志. \\ \
    \item 曾获高中全国物理竞赛省一等奖 (实验部分全省第 8 名), 自学大学与研究生阶段物理课程, 数理基础扎实.
  \end{itemize}
\end{frame}

\section{研究经历}

\subsection{偏微分方程的有限元方法研究}

\begin{frame}{有限元方法简介}
  Finite element space and grids:
  \begin{figure}[htbp]
    \centering
    \includegraphics[height=.5\textwidth]{figure/basis.png} \includegraphics[height=.5\textwidth]{figure/grid.jpg}
    \caption{Linear basis in 1D; Grids}
  \end{figure}

\end{frame}

\begin{frame}{主要内容}
  Equation:
  $$
    \begin{cases}
      \varepsilon^2\Delta^{2}u-\Delta u=f \quad\;\; \textrm{in}~\Omega, \\
      u=\partial_nu=0 \quad\quad\quad \textrm{on}~\partial\Omega,
    \end{cases}
  $$

  \begin{itemize}
    % \item Original variational form
    %       $$
    %         \varepsilon^2 (\nabla_h^{2}u_{h0}, \nabla_h^{2}v_h)+(\nabla_h u_{h0}, \nabla_h v_h)=(f, P_hv_h) \quad \forall~v_h \in V_{h0}.
    %       $$
    \item Original ways to solve: \\ \
          \begin{itemize}
            \item Conforming elements: computational expensive
            \item Non-conforming elements: isn't convergent
          \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{主要内容}
  Our work:
  \begin{itemize}
    \item Modified the right hand side via projection:
          $$
            \begin{aligned}
              \left(\nabla w_{h}, \nabla \chi_{h}\right)                                        & =\left(f, \chi_{h}\right)                    &  & \forall \chi_{h} \in W_{h} \\
              \varepsilon^{2} a_{h}\left(u_{h 0}, v_{h}\right)+b_{h}\left(u_{h 0}, v_{h}\right) & =\left(\nabla w_{h}, \nabla_{h} v_{h}\right) &  & \forall v_{h} \in V_{h 0}
            \end{aligned}
          $$
    \item Decoupled the left hand side into four simple equations: \\ \
          {\footnotesize
          $$
            \begin{aligned}
              \left(\operatorname{curl}_{h} z_{h}, \operatorname{curl}_{h} v_{h}\right)                                                                                & =\left(\nabla w_{h}, \nabla_{h} v_{h}\right)             &  & \forall v_{h} \in V_{h 0}          \\
              \left(\phi_{h}, \psi_{h}\right)+\varepsilon^{2}\left(\nabla_{h} \phi_{h}, \nabla_{h} \psi_{h}\right)+\left(\operatorname{div}_{h} \psi_{h}, p_{h}\right) & =\left(\operatorname{curl}_{h} z_{h}, \psi_{h}\right)    &  & \forall \psi_{h} \in V_{h 0}^{C R} \\
              \left(\operatorname{div}_{h} \phi_{h}, q_{h}\right)                                                                                                      & =0                                                       &  & \forall q_{h} \in \mathcal{Q}_{h}  \\
              \left(\operatorname{curl}_{h} u_{h 0}, \operatorname{curl}_{h} \chi_{h}\right)                                                                           & =\left(\phi_{h}, \operatorname{curl}_{h} \chi_{h}\right) &  & \forall \chi_{h} \in V_{h 0}
            \end{aligned}
          $$
          }
  \end{itemize}
\end{frame}

\begin{frame}{实验结果}
  \begin{itemize}
    \item Equations solved efficiently with the simplest elements.
    \item Final paper published in \textit{Journal of Scientific Computing}.
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{figure/itersteps.jpg}
    \caption{Robust iteration steps when solving}
  \end{figure}
\end{frame}

% \begin{frame}{总结}
%   \begin{itemize}
%     \item Final paper accepted by \textit{Journal of Scientific Computing} \\ \
%     \item Found a suitable open source package myself and studied many bottom-level codes.\\ \
%     \item Fixed many bugs in developing experiments by discussing. Reported bug for the package and contributed codes to develop it.
%   \end{itemize}
% \end{frame}

\subsection{预训练模型的可解释性研究}

\begin{frame}{研究背景}
  Existing methods
  \begin{itemize}
    \item Gradient based methods: gradient, dot product with embeddings, integrated gradient ...
    \item Perturbation based methods: input reduction, adversarial perturbations ... \\ \
  \end{itemize}
  Trying to explain the \textbf{relations between words} that model is learning during the training process.

    {\tiny
      $$
        \begin{aligned}
          Loss\left(x_{1}, \ldots, x_{d}\right)= & f\left(a_{1}, \ldots, a_{d}\right)+\sum_{j=1}^{d} \frac{\partial f\left(a_{1}, \ldots, a_{d}\right)}{\partial x_{j}}\left(x_{j}-a_{j}\right)                                                                                                   \\
          +                                      & \frac{1}{2 !} \sum_{j=1}^{d} \sum_{k=1}^{d} \frac{\partial^{2} f\left(a_{1}, \ldots, a_{d}\right)}{\partial x_{j} \partial x_{k}}\left(x_{j}-a_{j}\right)\left(x_{k}-a_{k}\right)                                                              \\
          +                                      & \frac{1}{3 !} \sum_{j=1}^{d} \sum_{k=1}^{d} \sum_{l=1}^{d} \frac{\partial^{3} f\left(a_{1}, \ldots, a_{d}\right)}{\partial x_{j} \partial x_{k} \partial x_{l}}\left(x_{j}-a_{j}\right)\left(x_{k}-a_{k}\right)\left(x_{l}-a_{l}\right)+\cdots
        \end{aligned}
      $$}
\end{frame}

% \begin{frame}{Dataset}
%   \begin{itemize}
%     \item e-SNLI dataset: essential words are highlighted by annotators
%   \end{itemize}
%   \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.9\textwidth]{figure/esnli.png}
%     \caption{Examples from e-SNLI}
%   \end{figure}
% \end{frame}

% \begin{frame}{Experiments}
%   \begin{itemize}
%     \item Loss durning training BERT-Base:
%   \end{itemize}
%   \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{figure/overfit_loss.png}
%   \end{figure}

% \end{frame}

% \begin{frame}{Experiments}
%   \begin{itemize}
%     \item Gradients of annotated "essential words" durning training BERT-Base:
%   \end{itemize}
%   \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{figure/overfit_grad.png}
%   \end{figure}
% \end{frame}

% \begin{frame}{Experiments}
%   \begin{itemize}
%     \item Ratio: $\frac {|\text{Gradients of annotated}|}{|\text{Gradients of all}|}$ durning training BERT-Base:
%   \end{itemize}
%   \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{figure/overfit_ratio.png}
%   \end{figure}
% \end{frame}

\begin{frame}{实验结果}
  \begin{itemize}
    \item Model learning relations between "essential" words?
    \item $\frac {\partial Loss}{\partial E_i E_j}$ durning training BERT-Base:
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{figure/cross_loss.png}\includegraphics[width=.5\textwidth]{figure/cross_ratio.png}
  \end{figure}
\end{frame}

% \begin{frame}{Experiments}
%   \begin{itemize}
%     \item What relations are captured? \\ \
%     \item Sentences (label: entailment): \begin{enumerate}
%             \item Two young boys of opposing teams play football, while wearing full protection uniforms and helmets.
%             \item Boys play football. \\ \
%           \end{enumerate}
%     \item 'essential words': $$\text{['boys', 'opposing', 'teams', 'play', 'boys', 'play', 'football']}$$
%   \end{itemize}
% \end{frame}

\begin{frame}{实验结果}
  \begin{itemize}
    \item Sentences: \begin{enumerate}
            \item Two young \textcolor{red}{boys of opposing teams play} football${}_1$, while wearing full protection uniforms and helmets.
            \item \textcolor{red}{Boys play football${}_2$}.
          \end{enumerate}
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\textwidth]{figure/batch0.png}
    \caption{At the beginning of training}
  \end{figure}
\end{frame}

\begin{frame}{实验结果}
  \begin{itemize}
    \item Sentences: \begin{enumerate}
            \item Two young \textcolor{red}{boys of opposing teams play} football${}_1$, while wearing full protection uniforms and helmets.
            \item \textcolor{red}{Boys play football${}_2$}.
          \end{enumerate}
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\textwidth]{figure/batch1.png}
    \caption{Trained for 200 batches}
  \end{figure}
\end{frame}

% \begin{frame}{Experiments}
%   \begin{itemize}
%     \item Sentences: \begin{enumerate}
%             \item Two young \textcolor{red}{boys of opposing teams play} football, while wearing full protection uniforms and helmets.
%             \item \textcolor{red}{Boys play football}.
%           \end{enumerate}
%   \end{itemize}
%   \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=.6\textwidth]{figure/batch2.png}
%     \caption{Trained for 1 epoch}
%   \end{figure}
% \end{frame}

% \begin{frame}{Future Work}
%   \begin{itemize}
%     \item Search for other dataset to simplify the problem. \\ \
%     \item Techniques to reduce computational cost (randomized?). \\ \
%     \item Design other experiments to figure out what is decreasing?
%   \end{itemize}
% \end{frame}

\subsection{Kaggle Question Answering 项目}

% \begin{frame}{Background}
%   \begin{itemize}
%     
%   \end{itemize}
% \end{frame}

\begin{frame}{项目介绍}
  \begin{block}{数据集介绍}
    \begin{itemize}
      \item Used the Natural Questions (NQ) (\textbf{Kwiatkowski et al., 2019}) dataset from Google AI.
      \item Each example is comprised of a google query and a corresponding Wikipedia page.
    \end{itemize}
  \end{block}
  \begin{block}{主要方案}
    \begin{itemize}
      \item Fine-tuning on SQuAD 2.0
      \item Mixed Precision Training
      \item Hard Negative Sampling
      \item Sifting candidates
    \end{itemize}
  \end{block}
\end{frame}

% \begin{frame}{Fine-tuning on SQuAD $2.0$}

%   \begin{itemize}
%     \item SQuAD $2.0$ - $130,000$ crowd sourced question and answer training pairs derived from Wikipedia paragraphs.
%   \end{itemize}

%   \begin{block}{Performance in Training}
%     \centering
%     $$\begin{array}{cccc}
%         \hline \text { Model }                & \text { Start\_acc } & \text { End\_acc } & \text { Class\_acc } \\
%         \hline \text { BERT base (Original) } & 59.1                 & 61.5               & 73.9                 \\
%         \text { ALBERT xlarge (Original) }    & 0.12                 & 0.13               & 66.67                \\
%         \text { ALBERT xlarge (Finetuned) }   & 82.54                & 86.37              & 85.75                \\
%         \hline
%       \end{array}$$
%     \quad \tiny{batch size = 3 per GPU, learning rate = 1e-5 for 3 epochs}
%   \end{block}
% \end{frame}

% \begin{frame}{Mixed Precision Training}

%   \begin{itemize}
%     \item To save memory and speed up - only had 1080Ti GPUs cluster.
%   \end{itemize}

%   \begin{block}{Experiment Result}
%     $$\begin{array}{cccc}
%         \hline \text { Precision}  & \text { EM } & \text { F1 } & \text{Speed up${}^{*}$} \\
%         \hline \text { FP32 only } & 84.86        & 88.00        & 1.0                     \\
%         \text { FP16 Only }        & 16.75        & 17.35        & 1.35\text{x}            \\
%         \text { Mixed precision }  & 84.94        & 87.97        & 1.05\text{x}            \\
%         \hline
%       \end{array}$$
%     \quad \tiny{$*$All tested during SQuAD2.0 fine-tuning }%and the speed will have more improvement on GPU with Tensor Cores}
%   \end{block}
% \end{frame}

% \begin{frame}{Hard Negative Sampling}
%   \begin{block}{Intuition}
%     \begin{itemize}
%       \item Training questions without any short answer($65\%$) $\rightarrow$ Too many negative Examples
%       \item Uniform sampling $\rightarrow$ most of the negative candidates are \textbf{"too easy"}
%       \item Hard negative sampling $\rightarrow$ increase the difficulty of training % the candidate-level
%     \end{itemize}
%   \end{block}
% \end{frame}

% \begin{frame}{Hard Negative Sampling}
%   \begin{block}{Procedure}
%     \begin{enumerate}
%       \item Train a model with uniform sampling and predict on the whole training data
%       \item Store the answer probability for each negative candidate
%       \item Normalize the probabilities within documents to form a distribution
%       \item Sample negative candidates from the probability distribution in training.
%     \end{enumerate}
%     \begin{figure}[htbp]
%       \centering
%       \includegraphics[width=0.6\textwidth]{figure/hardnegative.png}
%       % \caption{Hard Negative Sampling\label{qs}}
%     \end{figure}
%   \end{block}
% \end{frame}

% \begin{frame}{Hard Negative Sampling}
%   \begin{block}{Result}

%     \begin{itemize}
%       \item Result: the performance was improved $6.6\%$ on Public leaderboard and $9.8\%$ on Private leaderboard.
%     \end{itemize}

%     $$\begin{array}{ccc}
%         \hline \text { Model }                     & {\text { Public F1}} & {\text{ Private F1}} \\
%         \hline \text { BERT baseline }             & 0.516                & 0.482                \\
%         \text { BERT with Hard Negative Sampling } & 0.579                & 0.574                \\
%         \hline
%       \end{array}$$
%   \end{block}
% \end{frame}


% \begin{frame}{Sifting candidates}
%   \begin{block}{Sifting candidates with BERT base to reduce candidates}
%     \begin{enumerate}
%       \item First perform a full prediction on the validation set using a fast model (BERT Base) to reduce number of candidates.
%       \item Then use larger model to make predictions on the selected candidates.
%     \end{enumerate}
%   \end{block}

%   \begin{block}{Benefits}
%     \begin{enumerate}
%       \item Reduce much predicting time when adding large models.
%       \item More convenient to ensemble other models.
%     \end{enumerate}
%   \end{block}
% \end{frame}

% \begin{frame}{Sifting candidates}
%   \begin{block}{Performances}
%     $$\begin{array}{ccc}
%         \hline \text { Model }                                    & {\text { Public F1}} & {\text{ Private F1}} \\
%         \hline \text { BERT baseline }                            & 0.516                & 0.482                \\
%         \text { BERT Base (Hard Negative Sampling)}               & 0.579                & 0.574                \\
%         \text { BERT${}^{*}$ Sifted $\rightarrow$ ALBERT xlarge } & {0.640}              & {0.659}              \\
%         \hline
%       \end{array}$$
%     \quad \small{$*$ also trained with hard negative sampling}
%   \end{block}
% \end{frame}

\begin{frame}{实验结果}
  $$\begin{array}{ccc}
      \hline \text { Model }                                           & {\text { Public F1}} & {\text{ Private F1 }} \\
      \hline \text { Kaggle Best }                                     & 0.713                & 0.717                 \\
      \text { BERT Base baseline }                                     & 0.516                & 0.482                 \\
      \text { BERT Base (Hard Negative Sampling) }                     & 0.579                & 0.574                 \\
      \text { BERT Sifted{${}^{*}$} $\rightarrow$ ALBERT xlarge }      & \textbf{0.640}       & \textbf{0.659}        \\
      \text { Sifted $\rightarrow$ BERT Base+ALBERT\tiny{(ensemble)} } & 0.665                & 0.666                 \\
      \text {Sifted $\rightarrow$ BERT Large+ALBERT\tiny{(ensemble)} } & \textbf{0.738}       & \textbf{0.718}        \\
      \hline
    \end{array}$$
  \quad \small{$*$ Sifted here stands for first using BERT base to sift candidates}
\end{frame}

% \begin{frame}{Conclusion}
%   \begin{itemize}
%     \item Learned to search and read latest paper regularly and looked for many useful techniques.\\ \
%     \item Comfortable with Linux environment and commands, and also implementation of DL models. \\ \
%     \item Collaborated with group to discuss and do experiments together. Also asked for teachers' advice regularly.
%   \end{itemize}
% \end{frame}


\subsection{Kaggle Sentence Classification 项目}
\begin{frame}{主要内容}
  \begin{block}{数据集介绍}
    \begin{itemize}
      \item Complaints from citizens. The task is to predict label (200 classes in total).
    \end{itemize}
  \end{block}
  \begin{block}{主要方案}
    \begin{itemize}
      \item Pre-trained models on similar dataset THUCNews.
      \item Adopted \textbf{focal loss} for the long-tailed distributed labels.
      \item Designed a \textbf{auxiliary sentence pair task}.
      \item Also tried adversarial training, data augmentation, pesudo labels, adding other layers after BERT, using RoBERTa-wwm, ERNIE, etc.
    \end{itemize}
  \end{block}
\end{frame}

% \begin{frame}{Focal Loss}
%   \begin{itemize}
%     \item The 200 labels are long-tailed distributed.
%     \item Scaled losses according to how difficult the example is to predict.
%   \end{itemize}
%   \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figure/focal.png}
%     \caption{Different Loss Functions}
%     \label{fig:focal_label}
%   \end{figure}
% \end{frame}

\begin{frame}{辅助任务设计}
  \begin{itemize}
    \item The original classification task failed to utilize information in the labels.
    \item An auxiliary sentence pair task is then designed. \\ \
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figure/pairdata.png}
    \caption{Generating pair data}
    \label{fig:my_label}
  \end{figure}
\end{frame}

\begin{frame}{实验结果}
  \begin{itemize}
    \item Highest score in class (led by 0.4\%).
  \end{itemize}
  \begin{table}[htbp]
    \centering
    \caption{Selected Experiment Results}
    \begin{tabular}{ccc}
      \toprule
      Model                                                        & Public score & Private score \\
      \midrule
      ERNIE\footnote{Original Task: text classification}           & 0.7981       & 0.8000        \\
      ERNIE\footnote{Focal Loss}                                   & 0.7995       & 0.8030        \\
      ERNIE\footnote{Auxiliary Task: sentence pair classification} & 0.8049       & 0.8010        \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}


\subsection{金融知识图谱自动化构建项目}

\begin{frame}{项目简介}
  \begin{itemize}
    \item 与平安公司导师合作的应用型研究项目, 旨在帮助信贷面审人员快速识别申请人相关的潜在风险. \\ \
    \item 作为项目负责人, 每周组织组会学习分享实体识别, 关系抽取等知识图谱构建的关键技术. \\ \
    \item 使用 BERT 抽取最新资讯中涉及的风险事件, 并将风险标签与资讯文本共同输入 BERT-BiGRU, 进一步提升了实体识别任务的准确率.
  \end{itemize}
\end{frame}

\begin{frame}{项目简介}
  \begin{itemize}
    \item 完成了从数据的自动化爬取, 构建知识图谱到 Neo4j 图数据库存储和前端应用的完整流程.
    \item 被选为校``大学生创新创业计划''优秀项目, 并在学术论坛登台展示 \textbf{(2\%)}, 目前正进一步参加上海市相关竞赛.\\
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{figure/KG_tech.png}
  \end{figure}

\end{frame}

% \begin{frame}{Conclusion}
%   \begin{itemize}
%     \item Explored and implemented more useful techniques myself. \\ \
%     \item Faced with a more unpredictable project, designed an auxiliary task which performed "well". \\ \
%     \item Experimented more failed techniques and analyzed possible reasons.
%   \end{itemize}
% \end{frame}



\section{总结}
\begin{frame}{总结}
  \begin{itemize}
    \item 具有良好的数学基础和丰富 NLP 项目经验. \\ \
    \item 积极乐观热爱研究, 自学能力强能主动探索. \\ \
    \item 希望有机会能在中文信息处理实验室继续提升自己, 为中文文本的研究贡献力量.
  \end{itemize}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\textwidth]{figure/github.png}
  \end{figure}
\end{frame}

\begin{frame}
  \begin{center}
    {\Huge\calligra Thanks for your attention!}
  \end{center}
\end{frame}

\end{document}


